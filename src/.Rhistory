plot(x=df$international_students, y=df$income, xlab="Internation student %", ylab="Income")
pairs(~teaching+num_students+research+income+total_score, data=df)
pairs(~teaching+research+income+total_score, data=df)
qqnorm(df$total_score)
plot(x=df$teaching, y=df$research, xlab="Teaching", ylab="Research")
plot(x=df$teaching, y=df$income, xlab="Teaching", ylab="Income")
plot(x=df$international_students, y=df$income, xlab="Internation student %", ylab="Income")
pairs(~teaching+research+income+total_score, data=df)
qqnorm(df$total_score)
#Image Classification on Cifar 10
#Dependencies
install.packages("reticulate")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()
#Dependencies
install.packages("reticulate")
install.packages("reticulate")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
#install_tensorflow()
install_tensorflow(gpu = T)
install_tensorflow(version = "GPU")
install_tensorflow(version = "GPU")
#Dependencies
install.packages("reticulate")
install.packages("reticulate")
#Dependencies
install.packages("reticulate")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
devtools::install_github("rstudio/tensorflow")
#library(tensorflow)
#install_tensorflow()
#install_tensorflow(version = "GPU")
install_keras(tensorflow = "gpu")
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x[1:500,,,] / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y[1:500],num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x[1:50,,,]/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y[1:50],num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
model<-keras_model_sequential()
model %>% #works like a pipe i.e. model gets passed to the following code
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same", input_shape=c(32,32,3)) %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3)) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3) ) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer to calculate
#cross-entropy
layer_activation("softmax") #for computing Probabilities of classes
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=32, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=32, epochs=100,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y,num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y,num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
model<-keras_model_sequential()
model %>% #works like a pipe i.e. model gets passed to the following code
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same", input_shape=c(32,32,3)) %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3)) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3) ) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer to calculate
#cross-entropy
layer_activation("softmax") #for computing Probabilities of classes
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=32, epochs=1,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=128, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
library(tensorflow)
sessionInfo()
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x[1:500,,,] / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y[1:500],num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x[1:50,,,]/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y[1:50],num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
#Transfer learning through vgg16
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(32, 32, 3)
)
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
#Transfer learning through vgg16
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(48, 48, 3)
)
model <- keras_model_sequential() %>%
conv_base %>%
layer_flatten() %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
#freeze the vgg weights
freeze_weights(conv_base)
length(model$trainable_weights)
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=256, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
#Defining & Configuring the Model
#Using the Sequential model in keras
#Transfer learning through vgg16
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(32, 32, 3)
)
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y,num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y,num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
model<-keras_model_sequential()
model %>% #works like a pipe i.e. model gets passed to the following code
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same", input_shape=c(32,32,3)) %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3)) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3) ) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer to calculate
#cross-entropy
layer_activation("softmax") #for computing Probabilities of classes
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=256, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
View(cifar)
print("Hello, R!")
print("Hello, R!")
plot(1:10)
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
install.packages("vosonSML")
if (!"magrittr" %in% installed.packages()) install.packages("magrittr") ## this package allows you to use so-called pipe (%>%)
if (!"magrittr" %in% installed.packages()) install.packages("magrittr") ## this package allows you to use so-called pipe (%>%)
if (!"igraph" %in% installed.packages()) install.packages("igraph") ## this package is a network analysis tool
if (!"statnet" %in% install.packages()) install.packages("statnet") ## this package is another popular network analysis tool
Sys.which("make")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
Sys.which("make")
Sys.which("make")
Sys.which("make")
Sys.which("make")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
Sys.getenv("PATH")
Sys.setenv(PATH = paste(Sys.getenv("PATH"),
"C:\\RTools40",
"C:\\RTools40\\mingw64\\bin",
sep = ";"))
# Did it work (look at the end)?
Sys.getenv("PATH")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
# Now run the lines below to load the packages you have installed.
# You need to load packages every time you run the script or restart R.
library(magrittr)
library(igraph)
library(vosonSML)
# Did it work (look at the end)?
Sys.getenv("PATH")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML", dependencies = TRUE) ## this package is a social media data collection tool
library
library(RSiena)
library(statnet)
setwd("C:/Users/Omkar/Desktop/Northwestern/Fall20/Social Network Analytics/Assignments/Lab3")
# Read in data and convert to matrix format
friend.data.w1 <- as.matrix(read.table("s50-network1.dat"))
friend.data.w1
library(RSiena)
library(statnet)
library(dplyr)  # For data manipulation
library(intergraph)
setwd("S:/Python/Projects/SNA/Snap_Project/msia-490-snap-project/src")
sessionInfo() # check other attached packages.
list.files()
connectionsFromEdgesPre <- read.csv("../data/edges_pre.csv")
connectionsFromEdgesPost <- read.csv("../data/edges_all.csv")
# View the first rows of the edgelist to make sure it imported correctly:
head(connectionsFromEdgesPre)
head(connectionsFromEdgesPost)
net1 <- as.network.matrix(connectionsFromEdgesPre, matrix.type = "edgelist", directed = FALSE)
net1 <- as.matrix.network(net1)
net2 <- as.network.matrix(connectionsFromEdgesPost, matrix.type = "edgelist", directed = FALSE)
net2 <- as.matrix.network(net2)
net1
net2 <- as.matrix(net2)
net2
net2 <- as.matrix.network.adjacency(net2)
net2 <- edgelist_to_adjmat(net2)
net2 <- as.matrix(as.adjacency.matrix(net2))
net2 <- as.network.matrix(connectionsFromEdgesPost, matrix.type = "edgelist", directed = FALSE)
net2 <- as.matrix(as.adjacency.matrix(net2))
net2 <- as.network.matrix(connectionsFromEdgesPost, matrix.type = "edgelist", directed = FALSE)
net2 <- as.matrix(net2)
net2
net2 <- as.network.matrix(connectionsFromEdgesPost, matrix.type = "edgelist", directed = FALSE)
net2 <- as.matrix.network.adjacency(net2)
net2
net2 <- as.network.matrix(connectionsFromEdgesPost, matrix.type = "edgelist", directed = FALSE)
net2 <- as.matrix(as.matrix.network.adjacency(net2))
net2
net1 <- as.network.matrix(connectionsFromEdgesPre, matrix.type = "edgelist", directed = FALSE)
net1_g <- asIgraph(net1)
net2 <- as.network.matrix(connectionsFromEdgesPost, matrix.type = "edgelist", directed = FALSE)
net1_g
net1_g
net1_g <- get.adjacency(net1_g, sparse=FALSE)
net1_g <- as.matrix.network.adjacency(net1_g, sparse=FALSE)
net1_g <- as_adjacency_matrix(net1_g, sparse=FALSE)
net1_g <- as_adj(net1_g, sparse=FALSE)
library(igraph)
net1 <- as.network.matrix(connectionsFromEdgesPre, matrix.type = "edgelist", directed = FALSE)
net1_g <- asIgraph(net1)
net1_g <- as_adjacency_matrix(net1_g, sparse=FALSE)
net1_g
dim(net1_g)
setwd("C:/Users/Omkar/Desktop/Northwestern/Fall20/Social Network Analytics/Assignments/Lab3")
# Read in data and convert to matrix format
friend.data.w1 <- as.matrix(read.table("s50-network1.dat"))
friend.data.w1
setwd("S:/Python/Projects/SNA/Snap_Project/msia-490-snap-project/src")
net1 <- as.network.matrix(connectionsFromEdgesPre, matrix.type = "edgelist", directed = FALSE)
net1_g <- as_adjacency_matrix(net1, sparse=FALSE)
net1_g <- asIgraph(net1)
net1_g
net1_g <- as_adjacency_matrix(net1_g, sparse=FALSE)
net1_g
adj_pre
setwd("S:/Python/Projects/SNA/Snap_Project/msia-490-snap-project/src")
adj_pre <- as.matrix(read.table("../data/adj_pre.dat"))
adj_pre
adj_post <- as.matrix(read.table("../data/adj_post.dat"))
combined_net <- array(c(adj_pre, adj_post), dim=c(1505, 1505, 2))
sNet <- sienaNet(combined_net)
# Create report
myData <- sienaDataCreate(sNet)
print01Report(myData, modelname = 'report_init') ## this creates a file in your working directory
# Load the features
nu <- as.matrix(read.table("../data/nu.dat"))
eng <- as.matrix(read.table("../data/Engineer.dat"))
research <- as.matrix(read.table("../data/Research.dat"))
manager <- as.matrix(read.table("../data/Manager.dat"))
director <- as.matrix(read.table("../data/Director.dat"))
nu
nuNet <- - sienaNet(nu, type="behavior")
engNet <- - sienaNet(eng, type="behavior")
rNet <- - sienaNet(research, type="behavior")
mNet <- - sienaNet(manager, type="behavior")
dNet <- - sienaNet(director, type="behavior")
# Create report
myData <- sienaDataCreate(sNet, nuNet, engNet, rNet, mNet, dNet)
# Create report
# Model 1 effect at a time
myData <- sienaDataCreate(sNet, nuNet)
print01Report(myData, modelname = 'report_init') ## this creates a file in your working directory
myeff <- getEffects(mybehdata)
# Create report
# Model 1 effect at a time
mybehData <- sienaDataCreate(sNet, nuNet)
myeff <- getEffects(mybehdata)
# Create report
# Model 1 effect at a time
mybehdata <- sienaDataCreate(sNet, nuNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "nuNet")
mymodel <- sienaModelCreate(useStdInits = FALSE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE, returnDeps = TRUE)
# Create report
# Model 1 effect at a time - First nuNet (Whether users belong to northwestern or not)
mybehdata <- sienaDataCreate(sNet, nuNet, engNet, rNet, mNet, dNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "nuNet")
mymodel <- sienaModelCreate(useStdInits = FALSE, projname = 'sna_proj')
myeff
# Create report
# Model 1 effect at a time - First nuNet (Whether users belong to northwestern or not)
mybehdata <- sienaDataCreate(sNet, nuNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "nuNet")
mymodel <- sienaModelCreate(useStdInits = FALSE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE,
useCluster=TRUE, nbrNodes=4, returnDeps = TRUE)
mymodel <- sienaModelCreate(useStdInits = TRUE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE,
useCluster=TRUE, nbrNodes=4, returnDeps = TRUE)
ans1$theta
stan_err <- ans1$se
estimates <- abs(ans1$theta)
param_sig <- estimates/stan_err
odds_ratio = exp(ans1$theta)
probs = odds_ratio / (1 + odds_ratio)
# Option B:
library(texreg) ## if you haven't installed it, install it first using the command on the top of the script
# texreg(ans1) ## for LaTex
screenreg(ans1)
saveRDS(ans1, 'ans1_nu.rds')
param_sig
# Model 1 effect at a time - Second EngNet
mybehdata <- sienaDataCreate(sNet, engNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "engNet")
mymodel <- sienaModelCreate(useStdInits = TRUE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE,
useCluster=TRUE, nbrNodes=4, returnDeps = TRUE)
ans1$theta
stan_err <- ans1$se
estimates <- abs(ans1$theta)
param_sig <- estimates/stan_err
odds_ratio = exp(ans1$theta)
probs = odds_ratio / (1 + odds_ratio)
# Option B:
library(texreg) ## if you haven't installed it, install it first using the command on the top of the script
# texreg(ans1) ## for LaTex
screenreg(ans1)
ans1$theta
stan_err <- ans1$se
estimates <- abs(ans1$theta)
param_sig <- estimates/stan_err
odds_ratio = exp(ans1$theta)
probs = odds_ratio / (1 + odds_ratio)
# Option B:
library(texreg) ## if you haven't installed it, install it first using the command on the top of the script
# texreg(ans1) ## for LaTex
screenreg(ans1)
saveRDS(ans1, 'ans1_eng.rds')
# Model 1 effect at a time - Second EngNet
mybehdata <- sienaDataCreate(sNet, mNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "mNet")
mymodel <- sienaModelCreate(useStdInits = TRUE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE,
useCluster=TRUE, nbrNodes=4, returnDeps = TRUE)
screenreg(ans1)
saveRDS(ans1, 'ans1_MNet.rds')
# Model 1 effect at a time - Second EngNet
mybehdata <- sienaDataCreate(sNet, rNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "rNet")
mymodel <- sienaModelCreate(useStdInits = TRUE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE,
useCluster=TRUE, nbrNodes=4, returnDeps = TRUE)
screenreg(ans1)
# Model 1 effect at a time - Second EngNet
mybehdata <- sienaDataCreate(sNet, dNet)
myeff <- getEffects(mybehdata)
myeff$include[]
myeff <- includeEffects(myeff, egoX, altX, sameX, interaction1 = "dNet")
mymodel <- sienaModelCreate(useStdInits = TRUE, projname = 'sna_proj')
myeff
ans1 <- siena07(mymodel, data=mybehdata, effects=myeff, batch=FALSE, verbose=FALSE,
useCluster=TRUE, nbrNodes=4, returnDeps = TRUE)
screenreg(ans1)
saveRDS(ans1, 'ans1_dNet.rds')
