<<<<<<< Updated upstream
load("C:/Users/agarw/OneDrive/Desktop/NU_Notes/MSIA_490/SNAP/msia-490-snap-project/src/snap_file.RData")
load("C:/Users/agarw/OneDrive/Desktop/NU_Notes/MSIA_490/SNAP/msia-490-snap-project/data/.RData")
# Required packages
# igraph = Create graphs for network analysis
# StatNet = Perform network statistics
# Intergraph = Convert between graph objs. Network -> igraph and vice-versa
if (!"igraph" %in% installed.packages()) install.packages("igraph")
if (!"statnet" %in% install.packages()) install.packages("statnet")
if (!"intergraph" %in% install.packages()) install.packages("intergraph")
library(statnet)
library(dplyr)  # For data manipulation
library(intergraph)
=======
df = read.csv("C:/Users/Omkar/Desktop/Somaiya/L.Y/DWM/R Basics - Screenshots")
df = read.csv("C:/Users/Omkar/Desktop/Somaiya/L.Y/DWM/R Basics - Screenshots/housePred.sv")
df = read.csv("C:/Users/Omkar/Desktop/Somaiya/L.Y/DWM/R Basics - Screenshots/housePred.csv")
summary(df)
head(df, 10)
str(df)
colnames(df)
colnames(df)[1] = "Identifier"
colnames(df)
df = read.csv("C:/Users/Omkar/Desktop/Somaiya/L.Y/DWM/DP_Exp2/uniData.csv")
colnames(df)
summary(df)
colSums(is.na(df))
df[df == "-"] = NA
df[df == ""] = NA
summary(df)
colSums(is.na(df))
levels(df$total_score)
levels(df$total_score)[1] = NA
levels(df$total_score)
df[df == "-"] = NA
df[df == ""] = NA
summary(df)
colSums(is.na(df))
df$total_score = as.numeric(as.character(df$total_score))
summary(df$total_score)
df$income = as.numeric(as.character(df$income))
df$num_students = as.numeric(as.character(df$num_students))
df$international_students = as.numeric(sub("%", "", df$international_students))/100
df$international = as.numeric(as.character(df$international))
summary(df)
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
hist(df$total_score, xlab = "Total Score", col = "blue")
hist(df$teaching, xlabl = "Teaching", col = "blue")
hist(df$teaching, xlab = "Teaching", col = "blue")
hist(df$num_students, xlab = "Num Students", col= "blue")
hist(df$research, xlab = "Research", col = "blue")
hist(df$income, xlab = "Income", col = "blue")
hist(df$student_staff_ratio, xlab = "Student Staff Ratio", col = "blue")
mean_total_score = mean(na.omit(df$total_score))
mean_income = mean(na.omit(df$income))
mean_inter_students = mean(na.omit(df$international_students))
mean_inter = mean(na.omit(df$international))
median_num_students = median(na.omit(df$num_students))
median_studentStaff = median(na.omit(df$student_staff_ratio))
df_new = df
df_new$total_score[is.na(df_new$total_score)] = mean_total_score
df_new$income[is.na(df_new$income)] = mean_income
df_new$international[is.na(df_new$international)] = mean_inter
df_new$international_students[is.na(df_new$international_students)] = mean_inter_students
df_new$num_students[is.na(df_new$num_students)] = median_num_students
df_new$student_staff_ratio[is.na(df_new$student_staff_ratio)] = median_studentStaff
df_new$female_male_ratio = NULL
summary(df_new)
write.csv(df_new, file="C:/Users/Omkar/Desktop/Somaiya/L.Y/DWM/DP_Exp2/uniCleaned_40.csv", row.names = F)
boxplot(research~teaching, data=df, xlab="Teaching", ylab="Research")
plot(research~teaching, data=df, xlab="Teaching", ylab="Research")
plot(income~teaching, data=df, xlab="Teaching", ylab="Income")
plot(x=df$teaching, y=df$research, xlab="Teaching", ylab="Income")
plot(x=df$teaching, y=df$income, xlab="Teaching", ylab="Income")
plot(x=df$teaching, y=df$research, xlab="Teaching", ylab="Research")
plot(x=df$international_students, y=df$income, xlab="Internation student %", ylab="Income")
pairs(~teaching+num_students+research+income+total_score, data=df)
pairs(~teaching+research+income+total_score, data=df)
qqnorm(df$total_score)
plot(x=df$teaching, y=df$research, xlab="Teaching", ylab="Research")
plot(x=df$teaching, y=df$income, xlab="Teaching", ylab="Income")
plot(x=df$international_students, y=df$income, xlab="Internation student %", ylab="Income")
pairs(~teaching+research+income+total_score, data=df)
qqnorm(df$total_score)
#Image Classification on Cifar 10
#Dependencies
install.packages("reticulate")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()
#Dependencies
install.packages("reticulate")
install.packages("reticulate")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
#install_tensorflow()
install_tensorflow(gpu = T)
install_tensorflow(version = "GPU")
install_tensorflow(version = "GPU")
#Dependencies
install.packages("reticulate")
install.packages("reticulate")
#Dependencies
install.packages("reticulate")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("rstudio/keras")
library(keras)
devtools::install_github("rstudio/tensorflow")
#library(tensorflow)
#install_tensorflow()
#install_tensorflow(version = "GPU")
install_keras(tensorflow = "gpu")
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x[1:500,,,] / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y[1:500],num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x[1:50,,,]/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y[1:50],num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
model<-keras_model_sequential()
model %>% #works like a pipe i.e. model gets passed to the following code
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same", input_shape=c(32,32,3)) %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3)) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3) ) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer to calculate
#cross-entropy
layer_activation("softmax") #for computing Probabilities of classes
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=32, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=32, epochs=100,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y,num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y,num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
model<-keras_model_sequential()
model %>% #works like a pipe i.e. model gets passed to the following code
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same", input_shape=c(32,32,3)) %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3)) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3) ) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer to calculate
#cross-entropy
layer_activation("softmax") #for computing Probabilities of classes
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=32, epochs=1,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=128, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
library(tensorflow)
sessionInfo()
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x[1:500,,,] / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y[1:500],num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x[1:50,,,]/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y[1:50],num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
#Transfer learning through vgg16
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(32, 32, 3)
)
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
#Transfer learning through vgg16
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(48, 48, 3)
)
model <- keras_model_sequential() %>%
conv_base %>%
layer_flatten() %>%
layer_dense(units = 512, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
#freeze the vgg weights
freeze_weights(conv_base)
length(model$trainable_weights)
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=256, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
#Defining & Configuring the Model
#Using the Sequential model in keras
#Transfer learning through vgg16
conv_base <- application_vgg16(
weights = "imagenet",
include_top = FALSE,
input_shape = c(32, 32, 3)
)
library(keras) #load keras for further usage
?dataset_cifar10 #get to know more about this dataset through this in-built function
cifar<-dataset_cifar10() #download the in-built cifar10 dataset
#Training Data
train_x<-cifar$train$x / 255 #normalization of pixels
train_y<-to_categorical(cifar$train$y,num_classes = 10) #convert to one-hot encoding
#Test Data
test_x<-cifar$test$x/255 #normalization of pixels
test_y<-to_categorical(cifar$test$y,num_classes = 10) #convert to one-hot encoding
cat("No of training samples\t--", dim(train_x)[[1]], "\tNo of test samples\t--",dim(test_x)[[1]]) #row dimensions
#Defining & Configuring the Model
#Using the Sequential model in keras
model<-keras_model_sequential()
model %>% #works like a pipe i.e. model gets passed to the following code
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same", input_shape=c(32,32,3)) %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3)) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
layer_conv_2d(filter=48, kernel_size=c(3,3), padding="same") %>%
layer_activation("relu") %>%
layer_conv_2d(filter=48,kernel_size=c(3,3) ) %>%
layer_activation("relu") %>%
layer_max_pooling_2d(pool_size=c(2,2)) %>%
layer_dropout(0.25) %>%
#flatten the input
layer_flatten() %>%
layer_dense(512) %>%
layer_activation("relu") %>%
layer_dropout(0.5) %>%
#output layer-10 classes-10 units
layer_dense(10) %>%
#applying softmax nonlinear activation function to the output layer to calculate
#cross-entropy
layer_activation("softmax") #for computing Probabilities of classes
#Optimizer -rmsProp to do parameter updates
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
#Compiling the Model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = opt,
metrics = "accuracy"
)
#Summary of the Model and its Architecture
summary(model)
#TRAINING PROCESS OF THE MODEL
model %>% fit(
train_x, train_y, batch_size=256, epochs=10,
validation_data = list(test_x, test_y),
shuffle=TRUE
)
View(cifar)
print("Hello, R!")
print("Hello, R!")
plot(1:10)
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
install.packages("vosonSML")
if (!"magrittr" %in% installed.packages()) install.packages("magrittr") ## this package allows you to use so-called pipe (%>%)
if (!"magrittr" %in% installed.packages()) install.packages("magrittr") ## this package allows you to use so-called pipe (%>%)
if (!"igraph" %in% installed.packages()) install.packages("igraph") ## this package is a network analysis tool
if (!"statnet" %in% install.packages()) install.packages("statnet") ## this package is another popular network analysis tool
Sys.which("make")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
Sys.which("make")
Sys.which("make")
Sys.which("make")
Sys.which("make")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
Sys.getenv("PATH")
Sys.setenv(PATH = paste(Sys.getenv("PATH"),
"C:\\RTools40",
"C:\\RTools40\\mingw64\\bin",
sep = ";"))
# Did it work (look at the end)?
Sys.getenv("PATH")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML") ## this package is a social media data collection tool
# Now run the lines below to load the packages you have installed.
# You need to load packages every time you run the script or restart R.
library(magrittr)
library(igraph)
library(vosonSML)
# Did it work (look at the end)?
Sys.getenv("PATH")
if (!"vosonSML" %in% installed.packages()) install.packages("vosonSML", dependencies = TRUE) ## this package is a social media data collection tool
library
library(statnet)
library(dplyr)  # For data manipulation
library(intergraph)
setwd("S:/Python/Projects/SNA/Snap_Project/msia-490-snap-project/src")
>>>>>>> Stashed changes
sessionInfo() # check other attached packages.
list.files()
connectionsFromEdgesPre <- read.csv("../data/edges_all.csv")
# View the first rows of the edgelist to make sure it imported correctly:
head(connectionsFromEdgesPre)
# Convert the edgelist to a network object in statnet format:
# connections <- network(matrix(scan(file="edges_all.csv", skip=1), byrow=TRUE), matrix.type = "edgelist")
connections <- as.network.matrix(connectionsFromEdgesPre, matrix.type = "edgelist", directed = FALSE)
# Connections object in statnet format
conn_mat <- as.matrix.network(connections)
sum(conn_mat)
connections
# 'Omkar': 1, 'Aditya': 2, 'Amisha': 3, 'Anuradha': 4
# For coloring the connections, grab all connections separately for the 4 group members
c1 <- connections[,1]
c2 <- connections[,2]
c3 <- connections[,3]
c4 <- connections[,4]
# Color codes for just tied tied to one, or if tied to  others
total_nodes = dim(conn_mat)[1]
cCodes <- rep(NA, total_nodes)
for (i in 1:length(cCodes)) {
if ((c1[i] == 1) & (c2[i] == 0) & (c3[i] == 0) & (c4[i] == 0)) {
cCodes[i] <- "red"
}
if ((c1[i] == 0) & (c2[i] == 1) & (c3[i] == 0) & (c4[i] == 0)) {
cCodes[i] <- "blue"
}
if ((c1[i] == 0) & (c2[i] == 0) & (c3[i] == 1) & (c4[i] == 0)) {
cCodes[i] <- "yellow"
}
if ((c1[i] == 0) & (c2[i] == 0) & (c3[i] == 0) & (c4[i] == 1)) {
cCodes[i] <- "green"
}
if ((c1[i] + c2[i] + c3[i] + c4[i]) > 1 ) {
cCodes[i] <- "coral"
}
}
cCodes[1:4] <- "cyan"
# Add different attributes to the graph object
connections
detach(package:igraph)
<<<<<<< Updated upstream
#detach(package:igraph)  if igraph library is loaded
=======
>>>>>>> Stashed changes
# Color attribute
set.vertex.attribute(connections,"contact.color",cCodes)
connections
get.vertex.attribute(connections,"contact.color")
# Adding the "names" attribute
# Load the combined connections info file
combined_conn <- read.csv('../data/combined_data.csv')
# Extract the ID and Full Name column from it
# Note that %>% is used for piping functions
id_to_names <- combined_conn %>% select(ID, Full.Name)
# Delete duplicates
id_to_names = distinct(id_to_names, ID, .keep_all=TRUE)
# Sort the ids
id_to_names <- id_to_names[order(id_to_names$ID),]
# Pass the names as a vertex attribute
set.vertex.attribute(connections,"names", id_to_names$Full.Name)
# Also store the ID as the attribute
set.vertex.attribute(connections,"ID", id_to_names$ID)
par(mar = c(0, 0, 0, 0))
plot(connections, vertex.col = "contact.color")
<<<<<<< Updated upstream
# Converting statnet ogject into igraph
=======
# Visualize the giant component
>>>>>>> Stashed changes
connections_igraph <- asIgraph(connections)
connections_igraph
# Load igraph after finishing up with tasks related to intergraph
library('igraph')
par(mar = c(0, 0, 0, 0))  # This is to set plotting parameters
cCodes
V(connections_igraph)$color <- cCodes # Have to give valid R color names
color_mat = V(connections_igraph)$color
table(color_mat) # Get the color count
# Full pre-covid graph in a different layout
connections_igraph %>%
plot(.,
layout = layout_with_kk(.), ## Fruchterman-Reingold layout
edge.arrow.size = .4, ## arrow size
vertex.size = 5, ## node size
vertex.label = NA,
vertex.label.cex = .4, ## node label size
vertex.label.color = 'black') ## node label color
comp <- components(connections_igraph)
comp
# Giant component plot
giantGraph <- connections_igraph %>%
induced.subgraph(., which(comp$membership == which.max(comp$csize)))
vcount(giantGraph) ## the number of nodes/actors/users
ecount(giantGraph) ## the number of edges
par(mar = c(0, 0, 0, 0))
giantGraph %>%
plot(.,
layout = layout_with_kk(.), ## Davidson and Harel graph layout
edge.arrow.size = .4,
vertex.size = 6,
vertex.label = NA,
vertex.label.cex = .5,
vertex.label.color = 'black')
<<<<<<< Updated upstream
# Full pre-covid graph in a different layout
connections_igraph %>%
plot(.,
layout = layout_with_fr(.), ## Fruchterman-Reingold layout
edge.arrow.size = .4, ## arrow size
vertex.size = 5, ## node size
vertex.label = NA,
vertex.label.cex = .4, ## node label size
vertex.label.color = 'black') ## node label color
comp <- components(connections_igraph)
comp
# Giant component plot
giantGraph <- connections_igraph %>%
induced.subgraph(., which(comp$membership == which.max(comp$csize)))
vcount(giantGraph) ## the number of nodes/actors/users
ecount(giantGraph) ## the number of edges
par(mar = c(0, 0, 0, 0))
giantGraph %>%
plot(.,
layout = layout_with_kk(.), ## Davidson and Harel graph layout
edge.arrow.size = .4,
vertex.size = 6,
vertex.label = NA,
vertex.label.cex = .5,
vertex.label.color = 'black')
=======
>>>>>>> Stashed changes
sna_g <- igraph::get.adjacency(giantGraph, sparse=FALSE) %>% network::as.network.matrix()
names_attr <- vertex_attr(giantGraph, 'names')
ids_attr <- vertex_attr(giantGraph, 'ID')
# Save the giant-graph obj
save.image('PostCovid_image.RData')
load('PostCovid_image.RData')
detach(package:igraph)
is.directed(connections)
summary(connections)
network.size(connections)
length(isolates(connections))
network.density(connections)
# Calculate different centrality measures
library(statnet)
# Compute centralities based on 'network' package
# Calculate in-degree centrality
# Store the information
# Calculate degree centrality
# gmode = graph is used in case of undirected graphs
centralities <- data.frame( 'node_id' = ids_attr, 'node_name' = names_attr,
'degree' = degree(sna_g, gmode='graph', cmode = 'freeman'))
# Calculate betweenness centrality and store it in the data.frame called 'centralities'
centralities$betweenness <- betweenness(sna_g)
# Calculate closeness centrality and store it in the data.frame called 'centralities'
centralities$closeness <- igraph::closeness(giantGraph, mode = 'all')
# Calculate eigenvector centrality and store it in the data.frame called 'centralities'
centralities$eigen <- evcent(sna_g)
# Calculate Burt's network constraint and store it in the data.frame called 'centralities'
# using 'igraph' because 'sna' doesn't have the function
centralities$netconstraint <- igraph::constraint(giantGraph)
# Calculate authority and store it in the data.frame called 'centralities'
# using 'igraph' because 'sna' doesn't have the function
# 'igraph::' allows calling for any igraph function without loading the package
centralities$authority <- igraph::authority_score(giantGraph, scale = TRUE)$`vector`
# Calculate hub and store it in the data.frame called 'centralities'
# using 'igraph' because 'sna' doesn't have the function
centralities$hub <- igraph::hub_score(giantGraph, scale = TRUE)$`vector`
View(centralities)
<<<<<<< Updated upstream
detach('package:statnet', unload = TRUE)
library(igraph)
kcore <- giantGraph %>% graph.coreness(.) ## calculate k-cores
kcore ## show the results of k-core decomposition
# Count the number of nodes in each group
max_k <- max(kcore)
k_counts = rep(0, max_k)
for (i in max_k:1){
k_counts[i] = length(which(kcore == i))
}
k_counts
## Plot a graph colored by the k-core decomposition results
giantGraph %>%
plot(.,
layout = layout_with_lgl(.),
edge.arrow.size = .3,
vertex.size = 4,
vertex.color = adjustcolor(kcore, alpha.f = .3),
vertex.label.cex = .5,
vertex.label.color = 'black',
mark.groups = by(seq_along(kcore), kcore, invisible),
mark.shape = 1/4,
mark.col = rainbow(length(unique(kcore)),alpha = .1),
mark.border = NA
)
# Load the cluster
cluster = readRDS('cluster_ng_post.rds')
cluster
# modularity measure
modularity(cluster)
# Find the number of clusters
membership(cluster)   # affiliation list
length(cluster) # number of clusters
# Find the size the each cluster
# Note that communities with one node are isolates, or have only a single tie
sizes(cluster)
# Visualize clusters - that puts colored blobs around the nodes in the same community.
# You may want to remove vertex.label=NA to figure out what terms are clustered.
cluster %>% plot(.,giantGraph,
# layout = layout_with_gem(.),
layout = layout_with_fr(giantGraph),
edge.arrow.size = .3,
vertex.size = 4,
vertex.color = adjustcolor(membership(.), alpha.f = .3),
vertex.label.cex = .5,
vertex.label.color = 'black',
mark.groups = by(seq_along(membership(.)), membership(.), invisible),
mark.shape = 1/4,
mark.col = rainbow(length(.),alpha = .1),
# mark.border = NA
)
# Get the members of the 5th community
members_arr = cluster$membership
total_members = length(members_arr)
members_names = rep(0, sizes(cluster)[5])
counter = 1
for (i in 1:total_members){
if (members_arr[i] == 5)
{
members_names[counter] <- names_attr[i]
counter <- counter +  1
}
}
members_names
giantGraph %>% degree.distribution(.,) %>%
plot(., col = 'black', pch = 19, cex = 1.5,
main = 'Degree Distribution',
ylab = 'Density',
xlab = 'Degree')
# CCDF - Complementary Cumulative Distribution Function
# Plot a log-log plot of Degree distribution
giantGraph %>%
degree.distribution(.,cumulative = TRUE ) %>%
plot(1:(max(degree(giantGraph))+1),., ## since log doesn't take 0, add 1 to every degree
log='xy', type = 'l',
main = 'Log-Log Plot of Degree',
ylab = 'CCDF',
xlab = 'Degree')
# Fit a power law to the degree distribution
# The output of the power.law.fit() function tells us what the exponent of the power law is ($alpha)
# and the log-likelihood of the parameters used to fit the power law distribution ($logLik)
# Also, it performs a Kolmogov-Smirnov test to test whether the given degree distribution could have
# been drawn from the fitted power law distribution.
# The function thus gives us the test statistic ($KS.stat) and p-vaule ($KS.p) for that test
in_power <- giantGraph %>%
degree.distribution(.,) %>%
power.law.fit(.)
in_power
ntrials <- 1000 ## set a value for the repetition
cl.rg <- numeric(ntrials) ## create an estimated value holder for clustering coefficient
apl.rg <- numeric(ntrials) ## create an estimated value holder for average path length
for (i in (1:ntrials)) {
g.rg <- rewire(giantGraph, keeping_degseq(niter = 100))
cl.rg[i] <- transitivity(g.rg, type = 'global')
apl.rg[i] <- average.path.length(g.rg)
}
# plot a histogram of simulated values for clustering coefficient + the observed value
# Calculate the x-lim correctly to make sure the red line is also shown
x_low = min(cl.rg)
x_high = max(cl.rg)
epsilon = 0.005
hist(cl.rg,
main = 'Histogram of Clustering Coefficient',
xlab = 'Clustering Coefficient',
xlim = range(x_low, x_high+epsilon)
)
par(xpd = FALSE)
# the line indicates the mean value of clustering coefficient for your network
abline(v = giantGraph %>% transitivity(., type = 'global'), col = 'red', lty = 2)
# this tests whether the observed value is statistically different from the simulated distribution
t.test(cl.rg, mu=giantGraph %>% transitivity(., type = 'global'),
alternative = 'less') ##pick either 'less' or 'greater' based on your results
# plot a histogram of simulated values for average path length + the observed value
hist(apl.rg,
main = 'Histogram of Average Path Length',
xlab = 'Average Path Length')
# the line indicates the mean value of average path length for your network
abline(v = giantGraph %>% average.path.length(), col = 'red', lty = 2)
# this tests whether the observed value is statistically different from the simulated distribution
t.test(apl.rg, mu=giantGraph %>% average.path.length(.),
alternative = 'less') ##pick either 'less' or 'greater' based on your results
=======
>>>>>>> Stashed changes
